{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569237b5-267a-4eae-b70f-a64720989dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_markdown_files(src_dir, dest_dir):\n",
    "    \"\"\"递归地将所有子文件夹中的 Markdown 文件复制到目标目录\"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)  # 如果目标目录不存在，则创建\n",
    "\n",
    "    # 遍历源目录及其子文件夹\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):  # 只处理 .md 文件\n",
    "                print(file)\n",
    "                # 构造源文件路径\n",
    "                src_file = os.path.join(root, file)\n",
    "                # 构造目标文件路径\n",
    "                dest_file = os.path.join(dest_dir, file)\n",
    "                # 直接复制文件（如果目标文件存在，则会覆盖）\n",
    "                shutil.copy(src_file, dest_file)\n",
    "                print(f\"复制文件: {src_file} 到 {dest_file}\")\n",
    "\n",
    "# 使用示例\n",
    "src_directory = \"addition//\"  # 源目录路径\n",
    "dest_directory = \"outputs/\"  # 目标目录路径\n",
    "\n",
    "#copy_markdown_files(src_directory, dest_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e594ac3c-c8d4-4a81-800e-63e41c23e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm  # 进度条库\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\")\n",
    "]\n",
    "\n",
    "def process_markdown_file(file_path):\n",
    "    \"\"\"处理单个 Markdown 文件，返回 result1 和 result2\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            markdown_content = file.read()\n",
    "\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "        chunks = markdown_splitter.split_text(markdown_content)\n",
    "        \n",
    "        header_key = [chunks[i].metadata['Header'].lower() for i in range(2,len(chunks))]\n",
    "        index_1 = header_key.index('introduction')+2 if 'introduction' in header_key else None\n",
    "        index_1 = index_1 if index_1 else header_key.index('background')+2 if 'background' in header_key else None\n",
    "        index_2 = header_key.index('discussion') +2 if 'discussion' in header_key else None\n",
    "        #title = #chunks[0].metadata['Header'] if chunks[0].metadata else chunks[1].metadata['Header'] \n",
    "        title = 'none' \n",
    "\n",
    "        results1 = {\n",
    "            \"text\": chunks[index_1].page_content if index_1 is not None else ' ',\n",
    "            \"meta\": {\"title\": title, \"name\": file_path.split('/')[1], \"type\": \"introduction\"}\n",
    "        }\n",
    "        #results2 = {\n",
    "         #   \"text\": chunks[index_2].page_content if index_2 is not None else ' ',\n",
    "          #  \"meta\": {\"title\": title, \"name\": file_path.split('/')[1], \"type\": \"discussion\"}\n",
    "        #}\n",
    "        \n",
    "        return results1, results2\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def write_to_jsonl(results, output_file):\n",
    "    \"\"\"将结果写入 JSONL 文件\"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        for result in results:\n",
    "            json.dump(result, file, ensure_ascii=False)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def worker(args):\n",
    "    \"\"\"工作进程函数，将结果返回\"\"\"\n",
    "    file_path, result_list1, result_list2 = args\n",
    "    result1, result2 = process_markdown_file(file_path)\n",
    "    if result1 and result2:\n",
    "        result_list1.append(result1)\n",
    "        result_list2.append(result2)\n",
    "\n",
    "def process_files_in_parallel(input_dir, output_file1, output_file2, num_workers=4):\n",
    "    \"\"\"并行处理文件夹中的 Markdown 文件，分别写入两个 JSONL 文件\"\"\"\n",
    "    # 获取所有 Markdown 文件路径\n",
    "    markdown_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".md\")]\n",
    "\n",
    "    # 使用 Manager 创建共享列表\n",
    "    with Manager() as manager:\n",
    "        result_list1 = manager.list()\n",
    "        result_list2 = manager.list()\n",
    "        \n",
    "        # 创建进程池\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            # 使用 tqdm 包裹任务以显示进度条\n",
    "            with tqdm(total=len(markdown_files), desc=\"Processing files\") as pbar:\n",
    "                for _ in pool.imap_unordered(worker, [(f, result_list1, result_list2) for f in markdown_files]):\n",
    "                    pbar.update()\n",
    "        \n",
    "        # 将结果写入 JSONL 文件\n",
    "        write_to_jsonl(result_list1, output_file1)\n",
    "        write_to_jsonl(result_list2, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d338321-42b8-4656-90de-1f90f52d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_directory = 'all_md/'\n",
    "    output_jsonl1 = 'introduction.jsonl'\n",
    "    output_jsonl2 = 'discussion.jsonl'\n",
    "    \n",
    "    #process_files_in_parallel(input_directory, output_jsonl1, output_jsonl2, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b41e13f-ceee-4137-94c2-8e43f9a5f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import json\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm  \n",
    "from itertools import islice\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\")\n",
    "]\n",
    "def write_to_json(result, output_file):\n",
    "    \"\"\"将结果写入 JSONL 文件\"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        #for result in results:\n",
    "        json.dump(result, file, ensure_ascii=False)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3b44af-b18f-4361-b6c1-2939aa07d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_lines(input_string):\n",
    "    lines = input_string.splitlines()\n",
    "    processed_lines = []\n",
    "    pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    for line in lines:\n",
    "        if 'department of' in line.lower() or 'university' in line.lower() or 'PhD' in line or 'PhD,' in line or 'PhD.' in line or 'school of' in line.lower() or 'md,' in line.lower():\n",
    "            line = ''\n",
    "\n",
    "        line = re.sub(r\"(Received in original form|revised form|accepted|Published online)\", \"\", line)\n",
    "        line = re.sub(r\"(Journal of [A-Za-z\\s]+, Vol\\.\\s+\\d+,\\s+No\\.\\s+\\d+,\\s+[A-Za-z]+(?: \\d{4}), pp\\s+\\d+-DOI:\\s+\\d+\\.\\d+\\.\\w+).*?\", \"\", line)\n",
    "        # Remove content inside () and []\n",
    "        line = re.sub(r\"\\(.*?\\)|\\[.*?\\]\", \"\", line)\n",
    "        \n",
    "        # Remove content between vertical bars (|)\n",
    "        line = re.sub(r\"\\|.*?\\|\", \"\", line)\n",
    "        \n",
    "        # Remove sequences of ---\n",
    "        line = re.sub(r\"---+\", \"\", line)\n",
    "        \n",
    "        # Reduce multiple spaces to a single space\n",
    "        line = re.sub(r\"\\s{2,}\", \" \", line).strip()\n",
    "        \n",
    "        # Remove URLs\n",
    "        line = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", line)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        line = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"\", line)\n",
    "        \n",
    "        # Remove content enclosed in asterisks (**)\n",
    "        line = re.sub(r\"\\*{2}.*?\\*{2}\", \"\", line)\n",
    "        \n",
    "        # Remove content in parentheses\n",
    "        line = re.sub(r\"\\(.*?\\)\", \"\", line)\n",
    "        \n",
    "        \n",
    "        # Remove numeric patterns\n",
    "        #line = re.sub(r\"\\d+\\s+.*?\", \"\", line)\n",
    "        #line = re.sub(r\"\\d+\\s*\\.\\s*\\d*\\s*\\.\\s*\\d*\", \"\", line)\n",
    "        line = re.sub(r\"\\|<\\d+\\.\\d+\\s*\\|\", \"\", line)\n",
    "        line = re.sub(r\"\\|+\", \"\", line)\n",
    "        total_words = len(line.split())\n",
    "        if total_words > 20:\n",
    "            processed_lines.append(line)\n",
    "    \n",
    "    return \"\\n\".join(processed_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf93d71-ba0e-4988-ae8d-4b677ec0028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def remove_content(input_string):\n",
    "    \"\"\"\n",
    "    Remove parentheses () and square brackets [] along with their contents, remove newline characters,\n",
    "    remove content between vertical bars (|), remove sequences of ---, reduce multiple spaces to a single space,\n",
    "    remove URLs and email addresses, clear the text if newline ratio exceeds 5% of total words,\n",
    "    or if the ratio of '-' to total words exceeds 0.4, or if any line starts with a dash ('-'), \n",
    "    or if the ratio of numbers to total words exceeds 0.3, or remove content enclosed in asterisks (**).\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed string with brackets, their content, newline characters, content between vertical bars, --- removed,\n",
    "             extra spaces reduced, URLs and email addresses removed, and text cleared if specified conditions are met.\n",
    "    \"\"\"\n",
    "    # Check if any line starts with '-'\n",
    "    if any(line.strip().startswith('-') for line in input_string.splitlines()):\n",
    "        return \"\"  # Clear the text if any line starts with '-'\n",
    "    \n",
    "\n",
    "    result = process_lines(input_string)\n",
    "    result = result.replace(\"\\n\", \"\")\n",
    "    total_words = len(result.split())\n",
    "    dash_count = result.count(\"-\")\n",
    "    numeric_count = sum(c.isdigit() for c in result)\n",
    "\n",
    "    if total_words > 0 and ((result.count(\"\\n\") / total_words) > 0.05 or (dash_count / total_words) > 0.3 or (numeric_count / total_words) > 0.3):\n",
    "        return \"\"  # Clear the text if conditions are met\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "    \n",
    "#remove_brackets_content(chunks[87].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f09dcb-4dfd-426d-ab43-b669ab108307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(file_path, output_file):\n",
    "    num = 0\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        page_content = f.read()\n",
    "\n",
    "    markdown_document = page_content\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    for i, doc in enumerate(md_header_splits):\n",
    "        content = doc.page_content\n",
    "        token_num = num_tokens_from_string(content, \"o200k_base\")\n",
    "        results = {\"text\": content,\n",
    "                   \"meta\": {\"book_name\": file_path.split('/')[1],'num_token':token_num}\n",
    "        }\n",
    "        write_to_json(results, output_file)\n",
    "        num+=token_num\n",
    "        return num\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "#num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a53471b-331e-4a87-b0ff-96e9ed1e7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 33083/33083 [09:23<00:00, 58.73it/s]\n"
     ]
    }
   ],
   "source": [
    "nums = 0\n",
    "number = 0\n",
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "generator = os.walk(\"pretrained_data/outputs_books\")\n",
    "\n",
    "books = list(islice(generator,2))[0][2]\n",
    "\n",
    "for root, dirs, files in os.walk('all_md/'):\n",
    "    for file in tqdm(files):\n",
    "        contents = []\n",
    "        if file.endswith(\".md\"):\n",
    "            with open(f'all_md/{file}', \"r\", encoding='utf-8') as f:\n",
    "                page_content = f.read()\n",
    "\n",
    "                markdown_document = page_content\n",
    "\n",
    "                markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "                md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "                for i, doc in enumerate(md_header_splits if file in books else md_header_splits[1:-1]):\n",
    "                    content = remove_content(doc.page_content)\n",
    "                    token_num = num_tokens_from_string(content, \"o200k_base\")\n",
    "                    results = {\"text\": content,\n",
    "                               \"meta\": {\"book_name\": file,'num_token':token_num}\n",
    "                              }\n",
    "                    #nums+=token_num\n",
    "                    if token_num > 256:\n",
    "                        contents.append(results)\n",
    "                        nums+=token_num\n",
    "                    #print('-----------------')\n",
    "                with open('pretrained_data_cleaned.json', \"a\", encoding=\"utf-8\") as file:\n",
    "                    number +=len(contents)\n",
    "                    for result in contents:\n",
    "                        json.dump(result, file, ensure_ascii=False)\n",
    "                        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee64eee-3eb1-4f35-b83e-254f8dca855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113944622"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed962ac6-4e9b-4c3d-a4d5-3edc924108e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,3,4,5][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6726ebbd-1777-46ec-8a9a-3edcacfbc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "n = 0\n",
    "data = []\n",
    "with open('pretrained_data_cleaned.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            # 逐行加载 JSON 对象\n",
    "            json_object = json.loads(line.strip())\n",
    "            n+=json_object['meta']['num_token']\n",
    "            data.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {line}, error: {e}\")\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26de2d06-07ce-45c5-a8c2-ef3739f5ed2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182835"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d73b4e-87e6-4736-8d37-397d968fc93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113944622"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
