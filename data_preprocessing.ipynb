{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569237b5-267a-4eae-b70f-a64720989dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_markdown_files(src_dir, dest_dir):\n",
    "    \"\"\"递归地将所有子文件夹中的 Markdown 文件复制到目标目录\"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)  # 如果目标目录不存在，则创建\n",
    "\n",
    "    # 遍历源目录及其子文件夹\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):  # 只处理 .md 文件\n",
    "                print(file)\n",
    "                # 构造源文件路径\n",
    "                src_file = os.path.join(root, file)\n",
    "                # 构造目标文件路径\n",
    "                dest_file = os.path.join(dest_dir, file)\n",
    "                # 直接复制文件（如果目标文件存在，则会覆盖）\n",
    "                shutil.copy(src_file, dest_file)\n",
    "                print(f\"复制文件: {src_file} 到 {dest_file}\")\n",
    "\n",
    "# 使用示例\n",
    "src_directory = \"addition//\"  # 源目录路径\n",
    "dest_directory = \"outputs/\"  # 目标目录路径\n",
    "\n",
    "#copy_markdown_files(src_directory, dest_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594ac3c-c8d4-4a81-800e-63e41c23e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm  # 进度条库\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\")\n",
    "]\n",
    "\n",
    "def process_markdown_file(file_path):\n",
    "    \"\"\"处理单个 Markdown 文件，返回 result1 和 result2\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            markdown_content = file.read()\n",
    "\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "        chunks = markdown_splitter.split_text(markdown_content)\n",
    "        \n",
    "        header_key = [chunks[i].metadata['Header'].lower() for i in range(2,len(chunks))]\n",
    "        index_1 = header_key.index('introduction')+2 if 'introduction' in header_key else None\n",
    "        index_1 = index_1 if index_1 else header_key.index('background')+2 if 'background' in header_key else None\n",
    "        index_2 = header_key.index('discussion') +2 if 'discussion' in header_key else None\n",
    "        #title = #chunks[0].metadata['Header'] if chunks[0].metadata else chunks[1].metadata['Header'] \n",
    "        title = 'none' \n",
    "\n",
    "        results1 = {\n",
    "            \"text\": chunks[index_1].page_content if index_1 is not None else ' ',\n",
    "            \"meta\": {\"title\": title, \"name\": file_path.split('/')[1], \"type\": \"introduction\"}\n",
    "        }\n",
    "        #results2 = {\n",
    "         #   \"text\": chunks[index_2].page_content if index_2 is not None else ' ',\n",
    "          #  \"meta\": {\"title\": title, \"name\": file_path.split('/')[1], \"type\": \"discussion\"}\n",
    "        #}\n",
    "        \n",
    "        return results1, results2\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def write_to_jsonl(results, output_file):\n",
    "    \"\"\"将结果写入 JSONL 文件\"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        for result in results:\n",
    "            json.dump(result, file, ensure_ascii=False)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def worker(args):\n",
    "    \"\"\"工作进程函数，将结果返回\"\"\"\n",
    "    file_path, result_list1, result_list2 = args\n",
    "    result1, result2 = process_markdown_file(file_path)\n",
    "    if result1 and result2:\n",
    "        result_list1.append(result1)\n",
    "        result_list2.append(result2)\n",
    "\n",
    "def process_files_in_parallel(input_dir, output_file1, output_file2, num_workers=4):\n",
    "    \"\"\"并行处理文件夹中的 Markdown 文件，分别写入两个 JSONL 文件\"\"\"\n",
    "    # 获取所有 Markdown 文件路径\n",
    "    markdown_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".md\")]\n",
    "\n",
    "    # 使用 Manager 创建共享列表\n",
    "    with Manager() as manager:\n",
    "        result_list1 = manager.list()\n",
    "        result_list2 = manager.list()\n",
    "        \n",
    "        # 创建进程池\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            # 使用 tqdm 包裹任务以显示进度条\n",
    "            with tqdm(total=len(markdown_files), desc=\"Processing files\") as pbar:\n",
    "                for _ in pool.imap_unordered(worker, [(f, result_list1, result_list2) for f in markdown_files]):\n",
    "                    pbar.update()\n",
    "        \n",
    "        # 将结果写入 JSONL 文件\n",
    "        write_to_jsonl(result_list1, output_file1)\n",
    "        write_to_jsonl(result_list2, output_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d338321-42b8-4656-90de-1f90f52d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_directory = 'all_md/'\n",
    "    output_jsonl1 = 'introduction.jsonl'\n",
    "    output_jsonl2 = 'discussion.jsonl'\n",
    "    \n",
    "    process_files_in_parallel(input_directory, output_jsonl1, output_jsonl2, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b41e13f-ceee-4137-94c2-8e43f9a5f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import json\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm  # 进度条库\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\")\n",
    "]\n",
    "def write_to_json(result, output_file):\n",
    "    \"\"\"将结果写入 JSONL 文件\"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        #for result in results:\n",
    "        json.dump(result, file, ensure_ascii=False)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f09dcb-4dfd-426d-ab43-b669ab108307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(file_path, output_file):\n",
    "    num = 0\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        page_content = f.read()\n",
    "\n",
    "    markdown_document = page_content\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    for i, doc in enumerate(md_header_splits):\n",
    "        content = doc.page_content\n",
    "        token_num = num_tokens_from_string(content, \"o200k_base\")\n",
    "        results = {\"text\": content,\n",
    "                   \"meta\": {\"book_name\": file_path.split('/')[1],'num_token':token_num}\n",
    "        }\n",
    "        write_to_json(results, output_file)\n",
    "        num+=token_num\n",
    "        return num\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "#num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53471b-331e-4a87-b0ff-96e9ed1e7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▉                                                                  | 4683/33083 [01:18<07:09, 66.09it/s]"
     ]
    }
   ],
   "source": [
    "nums = 0\n",
    "number = 0\n",
    "for root, dirs, files in os.walk('all_md/'):\n",
    "    for file in tqdm(files):\n",
    "        contents = []\n",
    "        if file.endswith(\".md\"):\n",
    "            with open(f'all_md/{file}', \"r\", encoding='utf-8') as f:\n",
    "                page_content = f.read()\n",
    "\n",
    "                markdown_document = page_content\n",
    "\n",
    "                markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "                md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "                for i, doc in enumerate(md_header_splits):\n",
    "                    content = doc.page_content\n",
    "                    token_num = num_tokens_from_string(content, \"o200k_base\")\n",
    "                    results = {\"text\": content,\n",
    "                               \"meta\": {\"book_name\": file,'num_token':token_num}\n",
    "                              }\n",
    "                    nums+=token_num\n",
    "                    contents.append(results)\n",
    "                    #print('-----------------')\n",
    "                with open('pretrained_data.json', \"a\", encoding=\"utf-8\") as file:\n",
    "                    number +=len(contents)\n",
    "                    for result in contents:\n",
    "                        json.dump(result, file, ensure_ascii=False)\n",
    "                        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee64eee-3eb1-4f35-b83e-254f8dca855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25765870"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6726ebbd-1777-46ec-8a9a-3edcacfbc9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73140"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
